% -*- LaTeX -*-
% -*- coding: utf-8 -*-
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%                             michael a.g. aïvázis
%                      california institute of technology
%                      (c) 1998-2010  all rights reserved
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%

\lecture{Programming with MPI}{20100125}

% --------------------------------------
% template
\begin{frame}[fragile]
%
  \frametitle{Distributed memory parallelism}
%
  \begin{itemize}
%
  \item recall the generic layout of a distributed memory machine
%
    \begin{figure}
      \centering
      \includegraphics[scale=1.0]{figures/distributed-memory.pdf}
    \end{figure}
    \vspace{-1.0em}
%
    \begin{itemize}
      \item each processor has its own private memory space
      \item processors communicate via the interconnect substrate
    \end{itemize}
%
  \item the programming model
    \begin{itemize}
    \item program consists of a collection of $p$ named processes
      \item each process has its own instruction stream and address space
    \item logically shared data must be partitioned among the processors
    \item communication and synchronization must be orchestrated explicitly 
    \item processes communicate via explicit data exchanges
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% template
\begin{frame}[fragile]
%
  \frametitle{\mpi\ -- the survivor}
%
  \begin{itemize}
%
  \item the {\em de facto} standard for writing parallel programs using message passing
    \begin{itemize}
    \item a library of routines callable from almost any programming language
    \item that enables communication among multiple processes
    \item standardized and portable API with good implementations available for almost any kind
      of parallel computer
    \end{itemize}
%
  \item \mpi\ is large and complex
    \begin{itemize}
    \item more than 125 functions, lot's of options and communication protocols
    \item but for most practical purposes, a small subset will suffice
    \item short introduction today, more when we consider specific physics
    \end{itemize}
%
  \item two major versions available -- check your installation for compliance
    \begin{itemize}
    \item \mpi-1: parallel machine management, process groups, collective operations,
      point-to-point operations, virtual topologies, profiling
    \item \mpi-2: dynamic process management, one-sided operations, parallel I/O, (simplistic)
      bindings for \cpp
    \end{itemize}
%
  \item \identifier{openmpi}: currently the best open source implementation
    \begin{itemize}
    \item well-architected, thread safe, fast, decent support from a broad community
    \end{itemize}
  \end{itemize}
%
\end{frame}

% --------------------------------------
% compiling, linking, staging and launching
\begin{frame}[fragile]
%
  \frametitle{Getting started}
%
  \begin{itemize}
%
  \item compiling and linking:
    \begin{itemize}
    \item most \mpi\ implementation supply wrappers around the available compilers
      \begin{itemize}
      \item e.g.~\identifier{mpicc}, \identifier{mpic++}, \identifier{mpif77},
        \identifier{mpif90}
      \end{itemize}
    \item it's not magic, so you can do it on your own to
      \begin{itemize}
      \item override the system defaults (without upsetting the sysadmins...)
      \item build multiple versions so you can benchmark
      \end{itemize}
    \end{itemize}
%
  \item staging and launching:
    \begin{itemize}
    \item most implementations provide \identifier{mpirun} to
      \begin{itemize}
      \item control the total number of desired processes
      \item specify the hostnames of the machines to use
      \item specify the mapping of processes to machines/CPUs/cores
      \item establish the current working directory, if possible, for all processes
      \item launch the program
      \end{itemize}
%
    \item but most installations do not permit its use; they have queuing systems instead
      \begin{itemize}
      \item \identifier{PBS}, \identifier{LSF}, \identifier{torque}, \identifier{maui}, ...
      \item specified and documented in the ``welcome'' package of most supercomputer centers
      \item scheduling of jobs, guarantee exclusive access to your allocated machines,
        establish upper time limit, charge the right account for your uses
      \end{itemize}
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% initializing the runtime environment
\begin{frame}[fragile]
%
  \frametitle{At runtime}
%
  \begin{itemize}
%
%
  \item initializing the co\"operating processes:
    \begin{C}
int MPI_Init(int* argc, char ***argv);
    \end{C}
    \begin{itemize}
    \item note the strange signature; see \slideref{hello-world-mpi} for an example of its use
    \item some implementations -- notably \identifier{MPICH}, the reference implementation --
      used command line arguments to pass information from \identifier{mpirun} to the runtime
      environment
    \item so they need {\em write} access to the command line arguments to strip the extras
    \item thankfully, not done any more
    \end{itemize}
%
  \item must be the first \mpi\ in your program; nothing is initialized correctly until it
    returns
    \begin{itemize}
    \item if this call does not return \identifier{MPI\_SUCCESS}, you should abort
    \end{itemize}
% 
  \item don't forget to shut everything down:
    \begin{C}
int MPI_Finalize(void);
    \end{C}
%
  \item must be the last \mpi\ call in your program; nothing is in usable state after it
    returns
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% communicators
\begin{frame}[fragile]
%
  \frametitle{Groups and communicators}
%
  \begin{itemize}
%
  \item every \mpi\ process belongs to at least one {\em group}
%
  \item groups have associated {\em communicators} that provide the context for data exchanges and
      synchronization among processes
%
  \item processes in a given communicator get {\em ranked}
    \begin{itemize}
    \item a communicator of $p$ processes assigns ranks 0 through $p-1$
    \item a process can discover the communicator size and its own rank by using
      \begin{C}
int MPI_Comm_size(MPI_Comm communicator, int* size);
int MPI_Comm_rank(MPI_Comm communicator, int* rank);
      \end{C}
    \end{itemize}
% 
  \item the \mpi\ runtime environment creates the {\em global} communicator
    \begin{itemize}
    \item known as \identifier{MPI\_COMM\_WORLD}
    \item all processes are members
    \end{itemize}
% 
  \item it is good practice to learn to manage your own
    \begin{itemize}
    \item to narrow down global operations to processor subsets
    \item to promote {\em reuse}
    \item more details later
    \end{itemize}
% 
  \end{itemize}
%
\end{frame}

% --------------------------------------
% hello world
\begin{frame}[fragile]
%
  \frametitle{Hello world}
%
  \label{slide:hello-world-mpi}
%
  \begin{C}
#include <mpi.h>
#include <stdio.h>

int main(int argc, char* argv[]) {
    int status;
    int rank, size;

    /* initialize MPI */
    status = MPI_Init(&argc, &argv);
    if (status != MPI_SUCCESS) {
        printf("error in MPI_Init; aborting...\n");
        return status;
    }

    /* all good -- get process info and display it */
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    printf("hello from %03d/%03d!\n", rank, size);

    /* shut down MPI */
    MPI_Finalize();

    return 0;
}
  \end{C}
%
\end{frame}

% --------------------------------------
% messages
\begin{frame}[fragile]
%
  \frametitle{Messages}
%
  \begin{itemize}
%
  \item in general, data exchanges through MPI calls involve
    \begin{itemize}
    \item a communicator
      \begin{itemize}
      \item specifies which processes participate in the exchange
      \item resolves process ranks into processes
      \end{itemize}        
    \item {\em collective} operations involve the entire communicator
    \item {\em point-to-point} operations require the rank of the message source or destination
    \item the details of the message payload
      \begin{itemize}
      \item the address of the source buffer
      \item the data type of the buffer contents
      \item the number of items in the buffer
      \end{itemize}
    \end{itemize}
%
    \item \mpi\ provides some data abstractions to
      \begin{itemize}
      \item hide machine dependencies in the data representations to enhance portability and
        support heterogeneous clusters
      \item support user defined data types
      \item support non-contiguous data layouts
      \end{itemize}        
      
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% global operations
\begin{frame}[fragile]
%
  \frametitle{Collective operations: global reductions}
%
  \begin{itemize}
%
  \item {\em collective} operations involve all processes in a given communicator
%
  \item the \mpi\ version of our global reduction example uses
    \begin{C}
int MPI_Allreduce(
        void* send_buffer, void* recv_buffer,
        int count, MPI_Datatype datatype, MPI_Op operation,
        MPI_Comm communicator
        );
   \end{C}
%
  \item example legal values for \identifier{MPI\_Datatype}
    \begin{itemize}
    \item \cc: \identifier{MPI\_INT}, \identifier{MPI\_LONG}, \identifier{MPI\_DOUBLE} 
    \item \fortran: \identifier{MPI\_INTEGER}, \identifier{MPI\_DOUBLE\_PRECISION},
      \identifier{MPI\_COMPLEX}
    \end{itemize}
%
  \item legal values for \identifier{MPI\_Op}
    \begin{itemize}
    \item \identifier{MPI\_MAX}, \identifier{MPI\_MIN}, \identifier{MPI\_MAXLOC},
      \identifier{MPI\_MINLOC}
    \item \identifier{MPI\_SUM}, \identifier{MPI\_PROD}
    \item \identifier{MPI\_LAND}, \identifier{MPI\_LOR}, \identifier{MPI\_LXOR}
    \item \identifier{MPI\_BAND}, \identifier{MPI\_BOR}, \identifier{MPI\_BXOR}
    \item \identifier{MPI\_REPLACE}
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% example reduction with mpi
\begin{frame}[fragile]
%
  \frametitle{Example reduction using \mpi}
%
  \label{slide:squares-mpi}
%
  \begin{C}[basicstyle=\tt\bfseries\tiny]
#include <mpi.h>
#include <stdio.h>

int main(int argc, char* argv[]) {
    int status;
    int rank;
    int square, sum;

    /* initialize MPI */
    status = MPI_Init(&argc, &argv);
    if (status != MPI_SUCCESS) {
        printf("error in MPI_Init; aborting...\n");
        return status;
    }

    /* get the process rank */
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    /* form the square */
    square = rank*rank;
    /* each process contributes the square of its rank */
    MPI_Allreduce(&square, &sum, 1, MPI_INT,  MPI_SUM, MPI_COMM_WORLD);
    /* print out the result */
    printf("%03d: sum = %d\n", rank, sum);

    /* shut down MPI */
    MPI_Finalize();

    return 0;
}
  \end{C}
%
\end{frame}

% --------------------------------------
% sending and receiving messages
\begin{frame}[fragile]
%
  \frametitle{Point to point communication}
%
  \begin{itemize}
%
  \item to send a message
    \begin{C}
int MPI_Send(
        void* buffer, int count, MPI_Datatype datatype,
        int destination, int tag, MPI_Comm communicator
        );
   \end{C}
%
  \item to receive a message
    \begin{C}
int MPI_Recv(
        void* buffer, int count, MPI_Datatype datatype,
        int source, int tag, MPI_Comm communicator
        );
   \end{C}
%
  \item the \identifier{tag} enables choosing the order you may receive pending messages
%
  \item but for a given (\identifier{source},\identifier{tag},\identifier{communicator})
    messages are received in the order they were sent
%
  \item receiving via wildcards: \identifier{MPI\_ANY\_SOURCE} and \identifier{MPI\_ANY\_TAG}
% 
  \item in {\em standard} communication mode, sending and receiving messages are {\em blocking},
   so the function does not return until you can safely access the \identifier{buffer}
   \begin{itemize}
   \item to read, free, etc.
   \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% communication modes
\begin{frame}[fragile]
%
  \frametitle{Communication modes}
%
  \begin{itemize}
%
  \item in standard mode, the specification does not explicitly mention buffering strategy
    \begin{itemize}
    \item buffering messages would remove some of the access constraints but it requires time
      and storage for the multiple copies
    \item portability across implementations implies conservative assumptions about the order
      of initiation of sends and receives to avoid deadlock
    \end{itemize}
%
  \item in {\em ready} mode, you must post a receive before the matching send can be initiated
    \begin{itemize}
    \item \function{MPI\_Rsend}, \function{MPI\_Rrecv}
    \end{itemize}
%
  \item in {\em buffered} mode, sends can be initiated, and may complete, regardless of when
    the matching receive is initiate
    \begin{itemize}
    \item \function{MPI\_Bsend}, \function{MPI\_Brecv}
    \end{itemize}
%
  \item in {\em synchronous} mode, sends can be initiated regardless of whether the matching
    receive has been initiated, but the send will not return until the message has been
    received
    \begin{itemize}
    \item \function{MPI\_Ssend}, \function{MPI\_Srecv}
    \end{itemize}
  \end{itemize}
%
\end{frame}

% --------------------------------------
% asynchronous communications
\begin{frame}[fragile]
%
  \frametitle{Asynchronous communication}
%
  \begin{itemize}
%
  \item there are non-blocking versions of all these
    \begin{C}
int MPI_Isend(
        void* buffer, int count, MPI_Datatype datatype,
        int destination, int tag, 
        MPI_Comm communicator, MPI_Request* request
        );
    \end{C}
    \begin{itemize}
    \item faster, but you must take care to not access the message buffers until the messages
      have been delivered
    \item more details later in the course, as needed
    \end{itemize}
%
  \item for sends
    \begin{itemize}
    \item standard mode: \function{MPI\_Isend}
    \item ready mode: \function{MPI\_Irsend}
    \item buffered mode: \function{MPI\_Ibsend}
    \item synchronous mode: \function{MPI\_Issend}
    \end{itemize}
%
  \item only one call for receives: \function{MPI\_Irecv}
%
  \item extra \identifier{request} argument to check for completion of the request
    \begin{itemize}
    \item \function{MPI\_Test}, \function{MPI\_Wait} and their relatives
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% creating communicators and groups
\begin{frame}[fragile]
%
  \frametitle{Creating communicators and groups}
%
  \begin{itemize}
%
  \item communicators and groups are intertwined  
    \begin{itemize}
    \item you cannot create a group without a communicator
    \item you cannot create a communicator without a group
    \end{itemize}
%
  \item the cycle is broken by \identifier{MPI\_COMM\_WORLD}
    \begin{C}[basicstyle=\tt\bfseries\tiny]
#include <mpi.h>

int main(int argc, char* argv[]) {
    /* declare a communicator and a couple of groups */
    MPI_Comm workers;
    MPI_Group world_grp, workers_grp;

    /* initialize MPI; for brevity all status checks are omitted */
    MPI_Init(&argc, &argv);

    /* get the world communicator to build its group */
    MPI_Comm_group(MPI_COMM_WORLD, &world_grp);

    /* build another group by excluding a process */
    MPI_Group_excl(world_grp, 1, 0, &workers_grp);

    /* now build a communicator out of the processes in workers_grp */
    MPI_Comm_create(MPI_COMM_WORLD, worker_grp, &workers);

    /* etc.... */

    /* shut down MPI */
    MPI_Finalize();

    return 0;
}
    \end{C}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% freeing communicators and groups
\begin{frame}[fragile]
%
  \frametitle{Manipulating communicators and groups}
%
  \begin{itemize}
%
  \item releasing resources
    \begin{C}
int MPI_Group_free(MPI_Group* group);
int MPI_Comm_free(MPI_Comm* communicator);
int MPI_Comm_disconnect(MPI_Comm* communicator);
    \end{C}
%
  \item you can make a new group by adding or removing processes from an existing one
    \begin{C}
int MPI_Group_incl(
    MPI_Group grp, int n, int* ranks, MPI_Group* new_group);
int MPI_Group_excl(
    MPI_Group grp, int n, int* ranks, MPI_Group* new_group);
    \end{C}
%
  \item or by using set operations
    \begin{C}
int MPI_Group_union(
    MPI_Group grp1, MPI_Group grp2, MPI_Group* new_group);
int MPI_Group_intersection(
    MPI_Group grp1, MPI_Group grp2, MPI_Group* new_group);
int MPI_Group_difference(
    MPI_Group grp1, MPI_Group grp2, MPI_Group* new_group);
    \end{C}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% virtual topologies
\begin{frame}[fragile]
%
  \frametitle{Virtual topologies}
%
  \begin{itemize}
%
  \item 
%
  \end{itemize}
%
\end{frame}



% --------------------------------------
% timing
\begin{frame}[fragile]
%
  \frametitle{Timing}
%
  \begin{itemize}
%
  \item the function
    \begin{C}
double MPI_Wtime();
    \end{C}
    returns the time in seconds from some arbitrary time in the past
    \begin{itemize}
    \item guaranteed not to change only for the duration of the process
    \end{itemize}
%
  \item you can compute the elapsed time for any program segment by making calls at the
    beginning and the end and computing the difference
%
  \item no guarantees about synchronized clocks among different processes
%
  \item you can compute the clock resolution by using
    \begin{C}
double MPI_Wtick();
    \end{C}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% other collective operations
\begin{frame}[fragile]
%
  \frametitle{Other collective operations}
%
  \begin{itemize}
%
  \item \function{MPI\_Scan} computes partial reductions: the \th{p} process receives the
    result from processes 0 through $p-1$
    \begin{C}
int MPI_Scan(
        void* send_buffer, void* recv_buffer,
        int count, MPI_Datatype datatype, MPI_Op operation,
        MPI_Comm communicator
        );
   \end{C}
%
  \item \function{MPI\_Reduce} collects the result at only the given process \identifier{root}
    \begin{C}
int MPI_Reduce(
        void* send_buffer, void* recv_buffer,
        int count, MPI_Datatype datatype, MPI_Op operation,
        int root, MPI_Comm communicator
        );
   \end{C}
%
   \item synchronization is also a global operation:
    \begin{C}
int MPI_Barrier(MPI_Comm communicator);
   \end{C}
%
   participating processes block at a barrier until they have all reached it
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% scatter
\begin{frame}[fragile]
%
  \frametitle{Scatter}
%
  \begin{itemize}
%
  \item \function{MPI\_Scatter} sends data from \identifier{root} to all processes 
    \begin{C}
int MPI_Scatter(
        void* send_buffer, int send_count, MPI_Datatype send_datatype,
        void* recv_buffer, int recv_count, MPI_Datatype recv_datatype,
        int root, MPI_Comm communicator
        );
    \end{C}
    \begin{figure}
      \centering
      \includegraphics[scale=1.0]{figures/mpi-scatter.pdf}
    \end{figure}
%
  \item it is as if the data in \identifier{send\_buffer} were split in $p$ segments, and the
    \th{i} process receives the \th{i} segment
%
  \item the \identifier{send\_xxx} arguments are only meaningful for \identifier{root}; they
    are ignored for other processes
%
  \item the arguments \identifier{root} and \identifier{communicator} must be passed identical
    values by all processes
%
  \end{itemize}
% 
\end{frame}

% --------------------------------------
% gather
\begin{frame}[fragile]
%
  \frametitle{Gather}
%
  \begin{itemize}
%
  \item the converse is \function{MPI\_Gather} with \identifier{root} receiving data from all
    processes
    \begin{C}
int MPI_Gather(
        void* send_buffer, int send_count, MPI_Datatype send_datatype,
        void* recv_buffer, int recv_count, MPI_Datatype recv_datatype,
        int root, MPI_Comm communicator
        );
   \end{C}
   \begin{figure}
     \centering
     \includegraphics[scale=1.0]{figures/mpi-gather.pdf}
   \end{figure}
%
  \item it is as if $p$ messages, one from each processes, were concatenated in rank order and
    placed at \identifier{recv\_buffer}
%
  \item the \identifier{recv\_xxx} arguments are only meaningful for \identifier{root}; they
    are ignored for other processes
%
  \item the arguments \identifier{root} and \identifier{communicator} must be passed identical
    values by all processes
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% broadcasts
\begin{frame}[fragile]
%
  \frametitle{Broadcasting operations}
%
  \begin{itemize}
%
  \item \function{MPI\_Alltoall} sends data from all processes to all processes
    \begin{C}
int MPI_Alltoall(
        void* send_buffer, int send_count, MPI_Datatype send_datatype,
        void* recv_buffer, int recv_count, MPI_Datatype recv_datatype,
        MPI_Comm communicator
        );
   \end{C}
%
   a global scatter/gather
%
 \item use \function{MPI\_Bcast} to send the contents of a buffer from \identifier{root} to all
   processes in a communicator
   \begin{C}
int MPI_Bcast(
        void* buffer, int count, MPI_Datatype datatype,
        int root, MPI_Comm communicator
        );
   \end{C}
%
  \end{itemize}
%
\end{frame}

% end of file 
