% -*- LaTeX -*-
% -*- coding: utf-8 -*-
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%                             michael a.g. aïvázis
%                      california institute of technology
%                      (c) 1998-2010  all rights reserved
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%

\lecture{Matrices}{20100308}

% --------------------------------------
% triangular systems
\begin{frame}[fragile]
%
  \frametitle{Triangular systems}
%
  \begin{itemize}
%
  \item a matrix $L$ is lower triangular if all entries above the main diagonal are zero: $L_{ij} =
    0$ for $i < j$.
  \item a matrix $U$ is upper triangular if all entries below the main diagonal are zero: $U_{ij} =
    0$ for $i > j$.
  \item triangular systems appear frequently
    \begin{itemize}
    \item most direct methods of solving linear systems of equation start with a reduction of
      the matrix of coefficients into triangular form
    \item they are also used as preconditioners in iterative methods
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% forward substitution
\begin{frame}[fragile]
%
  \frametitle{Forward substitution}
%
  \begin{itemize}
%
  \item the lower triangular system $Lx = b$ can be solved by {\em forward substitution}
    \begin{equation}
      x_{i} = \left( b_{i} - \sum_{j=1}^{i-1} L_{ij}x_{j} \right) / L_{ii}
    \end{equation}
%
  \item that can be implemented as
    \begin{center}
      \begin{minipage}{.85\linewidth}
        \begin{algorithm}[H]
          \label{alg:forward-substitution}
%
          \dontprintsemicolon
          % \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \caption{\forwsub(L, b)}
%
          \For{$j=1$ \KwTo $n$}{
            $x_{j} = b_{j} / L_{jj}$ \;
            \For{$i=j+1$ \KwTo $n$}{
              $b_{i} = b_{i} - L_{ij} x_{j}$
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
  \item with roughly $n^{2}$ multiply-adds
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% backward substitution
\begin{frame}[fragile]
%
  \frametitle{Backward substitution}
%
  \begin{itemize}
%
%
  \item the upper triangular system $Ux = b$ can be solved by {\em backward substitution}
    \begin{equation}
      x_{i} = \left( b_{i} - \sum_{j=i+1}^{n} U_{ij}x_{j} \right) / U_{ii}
    \end{equation}
%
  \item that can be implemented as
    \begin{center}
      \begin{minipage}{.85\linewidth}
        \begin{algorithm}[H]
          \label{alg:backward-substitution}
%
          \dontprintsemicolon
          % \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \caption{\backsub(L, b)}
%
          \For{$j=n$ \KwTo $1$}{
            $x_{j} = b_{j} / U_{jj}$ \;
            \For{$i=1$ \KwTo $j-1$}{
              $b_{i} = b_{i} - U_{ij} x_{j}$
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
  \item with roughly $n^{2}$ multiply-adds
%
  \item the two algorithms are very similar, so focus on lower triangular systems
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% sequential implementations
\begin{frame}[fragile]
%
  \frametitle{Sequential implementations}
%
  \begin{itemize}
  \item there are two possible ways to arrange the forward substitution loops 
  \end{itemize}
%
  \begin{minipage}{.45\linewidth}
    \small
    \begin{algorithm}[H]
%
      \dontprintsemicolon
      \nocaptionofalgo
%
      \For{$j=1$ \KwTo $n$}{
        $x_{j} = b_{j} / L_{jj}$ \;
        \For{$i=j+1$ \KwTo $n$}{
          $b_{i} = b_{i} - L_{ij} x_{j}$
        }
      }
%
    \end{algorithm}
%
    \begin{itemize}
    \item immediate update
    \item data driven
    \item fan out
    \end{itemize}
%
  \end{minipage}
%
  \hfill
%
  \begin{minipage}{.45\linewidth}
    \small
    \begin{algorithm}[H]
%
      \dontprintsemicolon
      \nocaptionofalgo
%
      \For{$i=1$ \KwTo $n$}{
        \For{$j=1$ \KwTo $i-1$}{
          $b_{i} = b_{i} - L_{ij} x_{j}$
        }
        $x_{i} = b_{i} / L_{ii}$ \;
      }
%
    \end{algorithm}
%
    \begin{itemize}
    \item delayed update
    \item demand driven
    \item fan in
    \end{itemize}
%
  \end{minipage}
%
\end{frame}

% --------------------------------------
% parallelization
\begin{frame}[fragile]
%
  \frametitle{Parallelization}
%
  \begin{itemize}
%
  \item label the fine grain tasks as $(i,j)$ with $i,j = 1,\ldots,n$
    \begin{itemize}
    \item for $i=2,\ldots,n$ and $j=1,\ldots,i-1$, task $(i,j)$
      \begin{itemize}
      \item stores $L_{ij}$ 
      \item computes $L_{ij} x_{j}$
      \end{itemize}
    \item for $i=1,\ldots,n$, task $(i,i)$
      \begin{itemize}
      \item stores $L_{ii}$ and $b_{i}$
      \item collects the sum $t_{i} = \sum_{j=1}^{i-1} L_{ij}x_{j}$
        \item computes and stores $x+{i} = (b_{i} - t_{i})/ L_{ii}$
      \end{itemize}
    \end{itemize}
%
    \item this arrangement yields a two dimensional triangular grid of $n(n+1)/2$ fine grain tasks
%
    \item with the following communication patterns
      \begin{itemize}
      \item for $j=1,\ldots,n-1$, task $(j,j)$ broadcasts $x_{j}$ to tasks $(i,j)$,
        $i=j+1,\ldots,n$ 
      \item for $i=1,\ldots,n$, task $(i,i)$ collects the sum reduction of $L_{ij} x_{j}$ from
        tasks $(i,j)$, $j=1,\ldots,i-1$
      \end{itemize}
  \end{itemize}
%
\end{frame}

% --------------------------------------
% parallel implementation
\begin{frame}[fragile]
%
  \frametitle{Parallel implementation}
%
  \begin{itemize}
  \item here is the program for task $(i,j)$ with $i \geq j$
%
  \begin{center}
    \footnotesize
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \If{$i = j$}{
          \If{$i > 1$}{
            \KwRecv sum reduction $t$ \;
          } \Else {
            $t = 0$
          }
          \KwBcast $x_{i}$ \KwTo tasks $(k,i)$ and $(i,k)$, $k=i+1,\ldots,n$
        } \Else {
          \KwRecv $x_{j}$ \;
          $t = L_{ij} x_{j}$ \;
          \KwSend $t$ for reduction across tasks $(i,k)$, $k=1,\ldots,i-1$ to task $(i,i)$\;
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \item for properly pipelined communication, this algorithm can be implemented in $\order{n}$,
    but it requires $\order{n^{2}}$ tasks
  \item if there are many $b$ to solve for, the tasks can be working on multiple systems at
    the same time
  \item coarsening strategies can manage the number of tasks and improve the balance between
    computation and communication
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening
\begin{frame}[fragile]
%
  \frametitle{Coarsening by rows}
%
  \begin{itemize}
%
  \item for one dimensional coarsening into $n/p$ rows
    \begin{itemize}
    \item there is no need to communicate to perform the reductions, but also no parallelism
    \item vertical broadcasts are still needed to move the components of $x$
    \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \For{$j=1$ \KwTo $n$}{
          \If{ $j \in myrows$}{
            $x_{j} = b_{j} / L_{jj}$ \;
            \KwBcast $x_{j}$ to other tasks \;
          } \Else {
            \KwRecv $x_{j}$
          }
          \For{$i \in myrows$, $i > j$} {
            $b_{i} = b_{i} - L_{ij}/ x_{j}$
          }
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Observations on coarsening by rows}
%
  \begin{itemize}
%
  \item load balance:
%
    \begin{itemize}
    \item tasks become idle after the solution components corresponding to their last row are
      computed, and there is progressively more work as row number increases
    \item if a task holds a contiguous block of rows, it may become idle before mush of the
      calculation is finished
    \end{itemize}
%
  \item both concurrency and load balance may be improved by assigning rows to tasks in more
    creative ways
    \begin{itemize}
    \item cyclically: assign row $j$ to task $j \mod p$
    \item block cyclically
    \item reflectively
    \end{itemize}
%
  \item overall execution speed depends on the ability to overlap communication with the
    computation of successive steps
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Coarsening by columns}
%
  \begin{itemize}
%
  \item for one dimensional coarsening into $n/p$ rows
    \begin{itemize}
    \item there is no need to broadcast the components of $x$ vertically, but also no
      parallelism in computing the products
    \item horizontal exchanges are still required for the sum reductions that accumulate the
      inner products
    \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \For{$i=1$ \KwTo $n$}{
          t = 0 \;
          \For{ $j \in mycolumns$, $j < i$}{
            $t = t + L_{ij} x_{j}$
          }
          \If{ $i \in mycolumns$} {
            \KwRecv reduction of $t$ \;
            $x_{i} = (b_{i} -t)/ L_{ii}$
          } \Else {
            \KwSend $t$ for reduction across all tasks
          }
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% observations on coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Observations on coarsening by columns}
%
  \begin{itemize}
%
%
  \item load balance:
    \begin{itemize}
    \item tasks are idle until solution component corresponding to their first column is
      computed, and there is progressively less work as column number increases
    \item if a task holds a contiguous block of columns, it may remain idle for most of the
      calculation
    \end{itemize}
%
  \item both concurrency and load balance may be improved by assigning columns to tasks in more
    creative ways
    \begin{itemize}
    \item cyclically: assign column $j$ to task $j \mod p$
    \item block cyclically
    \item reflectively
    \end{itemize}
%
  \item overall execution speed depends on the ability to overlap communication with the
    computation of successive steps
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% wavefront algorithms
\begin{frame}[fragile]
%
  \frametitle{Wavefront algorithms}
%
  \begin{itemize}
%
  \item fan out and fan in algorithm share many characteristics
    \begin{itemize}
    \item parallelism comes from partitioning and distributing the work of the inner loop
    \item while the outer loop is serial
    \item they work on one component of the solution at a time, although one can partially
      pipeline successive steps
    \end{itemize}
%
    \item {\em wavefront} algorithms exploit parallelism in the outer loops by explicitly
      working on multiple components of the solution at the same time
%
    \item consider the one dimensional fan out algorithm
      \begin{itemize}
      \item it appears there is no opportunity for parallelism: after the owner of column $j$
        computes $x_{j}$, the updated components of $b$ cannot shared with other tasks because
        they have no access to column $j$
      \item however, the task that owns column $j$ could finish only a fraction of the work,
        say the first $s$ components, and pass them on to the task that owns column $j+1$,
        before continuing on to the next $s$ components
      \item the task that owns column $j+1$ receives the first $s$ components of $b$, it can
        compute $x_{j+1}$, begin a fraction of the remaining updates and forward its
        contributions to the next task
      \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% wavefront implementation
\begin{frame}[fragile]
%
  \frametitle{Wavefront implementation}
%
  \begin{itemize}
%
  \item we need two new features
    \begin{itemize}
    \item a vector $z$ that accumulates the updates to $b$
    \item the notion of a {\em segment} that contains no more than $s$ consecutive components of $z$
    \end{itemize}
%
  \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
          \dontprintsemicolon
          \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \For{$j \in mycolumns$}{
            \For{$k=1$ \KwTo number of segments}{
              \KwRecv $segment$\;
              \If {$k=1$} {
                $x_{j} = (b_{j} - z_{j})/ L_{jj}$ \;
                $segment = segment - \{z_{j}\}$ \;
              }
              \For{$z_{i} \in segment$} {
                $z_{i} = z_{i} + L_{ij}x_{j}$ \;
              }
              \If{$length(segment) > 0$}{
                \KwSend $segment$ \KwTo task owning column $j+1$
              }
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
\end{frame}



% --------------------------------------
% two dimensional coarsening
\begin{frame}[fragile]
%
  \frametitle{Block coarsening}
%
  \begin{itemize}
%
  \item coalesce $(n/\sqrt{p}) \times (n/\sqrt{p})$ fine grain tasks in to a coarse grain block
%
  \item resulting in an algorithm with features from both column and row coarsening
    \begin{itemize}
    \item both vertical broadcasts and horizontal reductions are required to communicate
      solution components and accumulate inner products
    \end{itemize}
%
  \item the na\"ive implementation assigns contiguous blocks of rows and columns to coarse
    tasks
    \begin{itemize}
    \item poor concurrency and efficiency 
    \item almost half the tasks are idle
    \end{itemize}
%
  \item cyclic assignment, with $L_{ij}$ assigned to task $(i \mod \sqrt{p}, j \mod \sqrt{p})$,
    yields $p$ non-null tasks so the full grid of tasks is active
    \begin{itemize}
    \item again, the obvious implementation where we loop over successive solution components
      to perform horizontal reductions and vertical broadcasts has limited concurrency, since
      computation of each component involves only one task row and one task column
    \item improve by computing solution components in groups of $\sqrt{p}$, which enables all
      tasks to perform the updates concurrently
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% iterative methods
\begin{frame}[fragile]
%
  \frametitle{Iterative methods}
%
  \begin{itemize}
%
  \item iterative methods start with an initial guess for the vector $x$ and improve until some
    desired accuracy is achieved
    \begin{itemize}
    \item no upper bound on the number of iterations to the exact solution
    \item in practice, establish an error measure such as $||b - Ax|| < \epsilon$
    \item particularly good with sparse matrices because sparsity is preserved
    \end{itemize}
  \item we will take a quick look at
    \begin{itemize}
    \item Jacobi
    \item Gauss-Seidel
    \item successive over-relaxation (SOR)
    \item conjugate gradient
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% template
\begin{frame}[fragile]
%
  \frametitle{The Jacobi method}
%
  \begin{itemize}
%
  \item given an initial guess $x^{(0)}$, the Jacobi method iterates by
    \begin{equation}
      x_{i}^{(k+1)} = \left( b_{i} - \sum_{j\neq i} A_{ij}x_{j}^{(k)} \right) / A_{ii}
    \end{equation}
    which can be expressed as
    \begin{equation}
      x^{(k+1)} = D^{-1} \left( b - (L+U)x^{(k)} \right)
    \end{equation}
    for $D$, $L$, and $U$ respectively diagonal, upper and lower triangular matrices
%
    \item the method requires
      \begin{itemize}
      \item non-zero diagonal entries, usually achievable by permuting rows
      \item extra storage for the $x$ iterates
      \end{itemize}
%
    \item convergence is neither guaranteed nor fast, but the components of $x^{(k)}$ are
      decoupled form each other so they can be computed in parallel
% 
  \end{itemize}
%
\end{frame}

% --------------------------------------
% gauss seidel
\begin{frame}[fragile]
%
  \frametitle{The Gauss-Seidel method}
%
  \begin{itemize}
%
  \item the convergence rate can be improved by using the components of $x^{(k+1)}$ as soon as
    they become available
    \begin{equation}
      x_{i}^{(k+1)} =
      \left(
        b_{i} - \sum_{j < i} A_{ij}x_{j}^{(k+1)} - \sum_{j > i} A_{ij}x_{j}^{(k)}
      \right) / A_{ii}
    \end{equation}
    or
    \begin{equation}
      x^{(k+1)} = (D+L)^{-1} \left( b - Ux^{(k)} \right)
    \end{equation}
%
  \item this  method also requires non-zero diagonal entries, but no extra storage for $x$
    since the values can be written in place
    \begin{itemize}
    \item but this coupling reduces the parallelism opportunities
    \end{itemize}
%
  \item convergence is about twice as fast as Jacobi, and guaranteed to converge under weaker
    conditions
    \begin{itemize}
    \item e.g.~positive definite symmetric $A$
    \item but may still be too slow for practical purposes
    \end{itemize}
% 
  \end{itemize}
%
\end{frame}

% --------------------------------------
% successive over-relaxation
\begin{frame}[fragile]
%
  \frametitle{Successive over-relaxation}
%
  \begin{itemize}
%
  \item SOR uses the weighted average of the current iterate and the next Gauss-Seidel iterate
    \begin{equation}
      x^{(k+1)} = (1-\omega) x^{(k)} + \omega x^{(k+1)}_{GS}
    \end{equation}
%
    where $\omega$ is relaxation parameter chosen to accelerate convergence
    \begin{itemize}
    \item $\omega > 1$ gives over-relaxation, $\omega < 1$ gives under-relaxation
    \item $\omega = 1$ is pure Gauss-Seidel
    \item the method diverges unless $0 < \omega < 2$
    \end{itemize}
%
  \item with  optimal value of $\omega$, the convergence rate is an order of magnitude faster
    than Gauss-Seidel
    \begin{itemize}
    \item but the optimal $\omega$ is difficult to find in general
    \end{itemize}
%
  \item this method suffers from reduced parallelism as well
    \begin{itemize}
    \item but allowing each process to use its most current value, rather than waiting for the
      latest update, leads to {\em asynchronous} over-relaxation
    \item but the stochastic nature complicates the convergence analysis
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% conjugate gradients
\begin{frame}[fragile]
%
  \frametitle{Conjugate gradients}
%
  \begin{itemize}
%
  \item another approach is to observe that, if $A$ is a positive definite matrix, the
    quadratic form
    \begin{equation}
      \phi(x) = \frac{1}{2} x^{T} A x - x^{T} b \label{eq:cg-qform}
    \end{equation}
    is minimized by the solution to the linear system $Ax = b$
  \item this optimization problem is solved by iterates 
    \begin{equation}
      x^{(k+1)} = x^{(k)} + \omega s^{(k)}
    \end{equation}
    where $\omega$ is a search parameter chosen to minimize the {\em objective function}
    $\phi(x^{(k)} + \omega s^{(k)})$ along the direction $s^{(k)}$
  \item {\em steepest descent} if obtained when $s^{(k)} = - \nabla \phi(x)$
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% implementation
\begin{frame}[fragile]
%
  \frametitle{Conjugate gradients for linear systems}
%
  \begin{itemize}
%
  \item for the special case of the quadratic problem in \eqref{cg-qform}
    \begin{itemize}
    \item the residual vector is the negative gradient
      \begin{equation}
        r = b - Ax = - \nabla \phi
      \end{equation}
    \item the optimal line search parameter is given by
      \begin{equation}
        \omega = \frac{r^{T} s}{s^{T} A s}
      \end{equation}
    \item successive search directions can be made orthogonal to $A$ by a simple three-term
      recurrence relation
    \end{itemize}
  \item leading to the following conjugate gradient algorithm for linear systems
%
  \end{itemize}
%
\end{frame}

% end of file 
