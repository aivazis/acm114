% -*- LaTeX -*-
% -*- coding: utf-8 -*-
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%                             michael a.g. aïvázis
%                      california institute of technology
%                      (c) 1998-2010  all rights reserved
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%

\lecture{Matrices}{20100308}

% --------------------------------------
% triangular systems
\begin{frame}[fragile]
%
  \frametitle{Triangular systems}
%
  \begin{itemize}
%
  \item a matrix $L$ is lower triangular if all entries above the main diagonal are zero: $L_{ij} =
    0$ for $i < j$.
  \item a matrix $U$ is upper triangular if all entries below the main diagonal are zero: $U_{ij} =
    0$ for $i > j$.
  \item triangular systems appear frequently
    \begin{itemize}
    \item most direct methods of solving linear systems of equation start with a reduction of
      the matrix of coefficients into triangular form
    \item they are also used as preconditioners in iterative methods
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% forward substitution
\begin{frame}[fragile]
%
  \frametitle{Forward substitution}
%
  \begin{itemize}
%
  \item the lower triangular system $Lx = b$ can be solved by {\em forward substitution}
    \begin{equation}
      x_{i} = \left( b_{i} - \sum_{j=1}^{i-1} L_{ij}x_{j} \right) / L_{ii}
    \end{equation}
%
  \item that can be implemented as
    \begin{center}
      \begin{minipage}{.85\linewidth}
        \begin{algorithm}[H]
          \label{alg:forward-substitution}
%
          \dontprintsemicolon
          % \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \caption{\forwsub(L, b)}
%
          \For{$j=1$ \KwTo $n$}{
            $x_{j} = b_{j} / L_{jj}$ \;
            \For{$i=j+1$ \KwTo $n$}{
              $b_{i} = b_{i} - L_{ij} x_{j}$
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
  \item with roughly $n^{2}$ multiply-adds
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% backward substitution
\begin{frame}[fragile]
%
  \frametitle{Backward substitution}
%
  \begin{itemize}
%
%
  \item the upper triangular system $Ux = b$ can be solved by {\em backward substitution}
    \begin{equation}
      x_{i} = \left( b_{i} - \sum_{j=i+1}^{n} U_{ij}x_{j} \right) / U_{ii}
    \end{equation}
%
  \item that can be implemented as
    \begin{center}
      \begin{minipage}{.85\linewidth}
        \begin{algorithm}[H]
          \label{alg:backward-substitution}
%
          \dontprintsemicolon
          % \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \caption{\backsub(L, b)}
%
          \For{$j=n$ \KwTo $1$}{
            $x_{j} = b_{j} / U_{jj}$ \;
            \For{$i=1$ \KwTo $j-1$}{
              $b_{i} = b_{i} - U_{ij} x_{j}$
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
  \item with roughly $n^{2}$ multiply-adds
%
  \item the two algorithms are very similar, so focus on lower triangular systems
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% sequential implementations
\begin{frame}[fragile]
%
  \frametitle{Sequential implementations}
%
  \begin{itemize}
  \item there are two possible ways to arrange the forward substitution loops 
  \end{itemize}
%
  \begin{minipage}{.45\linewidth}
    \small
    \begin{algorithm}[H]
%
      \dontprintsemicolon
      \nocaptionofalgo
%
      \For{$j=1$ \KwTo $n$}{
        $x_{j} = b_{j} / L_{jj}$ \;
        \For{$i=j+1$ \KwTo $n$}{
          $b_{i} = b_{i} - L_{ij} x_{j}$
        }
      }
%
    \end{algorithm}
%
    \begin{itemize}
    \item immediate update
    \item data driven
    \item fan out
    \end{itemize}
%
  \end{minipage}
%
  \hfill
%
  \begin{minipage}{.45\linewidth}
    \small
    \begin{algorithm}[H]
%
      \dontprintsemicolon
      \nocaptionofalgo
%
      \For{$i=1$ \KwTo $n$}{
        \For{$j=1$ \KwTo $i-1$}{
          $b_{i} = b_{i} - L_{ij} x_{j}$
        }
        $x_{i} = b_{i} / L_{ii}$ \;
      }
%
    \end{algorithm}
%
    \begin{itemize}
    \item delayed update
    \item demand driven
    \item fan in
    \end{itemize}
%
  \end{minipage}
%
\end{frame}

% --------------------------------------
% parallelization
\begin{frame}[fragile]
%
  \frametitle{Parallelization}
%
  \begin{itemize}
%
  \item label the fine grain tasks as $(i,j)$ with $i,j = 1,\ldots,n$
    \begin{itemize}
    \item for $i=2,\ldots,n$ and $j=1,\ldots,i-1$, task $(i,j)$
      \begin{itemize}
      \item stores $L_{ij}$ 
      \item computes $L_{ij} x_{j}$
      \end{itemize}
    \item for $i=1,\ldots,n$, task $(i,i)$
      \begin{itemize}
      \item stores $L_{ii}$ and $b_{i}$
      \item collects the sum $t_{i} = \sum_{j=1}^{i-1} L_{ij}x_{j}$
        \item computes and stores $x+{i} = (b_{i} - t_{i})/ L_{ii}$
      \end{itemize}
    \end{itemize}
%
    \item this arrangement yields a two dimensional triangular grid of $n(n+1)/2$ fine grain tasks
%
    \item with the following communication patterns
      \begin{itemize}
      \item for $j=1,\ldots,n-1$, task $(j,j)$ broadcasts $x_{j}$ to tasks $(i,j)$,
        $i=j+1,\ldots,n$ 
      \item for $i=1,\ldots,n$, task $(i,i)$ collects the sum reduction of $L_{ij} x_{j}$ from
        tasks $(i,j)$, $j=1,\ldots,i-1$
      \end{itemize}
  \end{itemize}
%
\end{frame}

% --------------------------------------
% parallel implementation
\begin{frame}[fragile]
%
  \frametitle{Parallel implementation}
%
  \begin{itemize}
  \item here is the program for task $(i,j)$ with $i \geq j$
%
  \begin{center}
    \footnotesize
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \If{$i = j$}{
          \If{$i > 1$}{
            \KwRecv sum reduction $t$ \;
          } \Else {
            $t = 0$
          }
          \KwBcast $x_{i}$ \KwTo tasks $(k,i)$ and $(i,k)$, $k=i+1,\ldots,n$
        } \Else {
          \KwRecv $x_{j}$ \;
          $t = L_{ij} x_{j}$ \;
          \KwSend $t$ for reduction across tasks $(i,k)$, $k=1,\ldots,i-1$ to task $(i,i)$\;
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \item for properly pipelined communication, this algorithm can be implemented in $\order{n}$,
    but it requires $\order{n^{2}}$ tasks
  \item if there are many $b$ to solve for, the tasks can be working on multiple systems at
    the same time
  \item coarsening strategies can manage the number of tasks and improve the balance between
    computation and communication
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening
\begin{frame}[fragile]
%
  \frametitle{Coarsening by rows}
%
  \begin{itemize}
%
  \item for one dimensional coarsening into $n/p$ rows
    \begin{itemize}
    \item there is no need to communicate to perform the reductions, but also no parallelism
    \item vertical broadcasts are still needed to move the components of $x$
    \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \For{$j=1$ \KwTo $n$}{
          \If{ $j \in myrows$}{
            $x_{j} = b_{j} / L_{jj}$ \;
            \KwBcast $x_{j}$ to other tasks \;
          } \Else {
            \KwRecv $x_{j}$
          }
          \For{$i \in myrows$, $i > j$} {
            $b_{i} = b_{i} - L_{ij}/ x_{j}$
          }
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Observations on coarsening by rows}
%
  \begin{itemize}
%
  \item load balance:
%
    \begin{itemize}
    \item tasks become idle after the solution components corresponding to their last row are
      computed, and there is progressively more work as row number increases
    \item if a task holds a contiguous block of rows, it may become idle before mush of the
      calculation is finished
    \end{itemize}
%
  \item both concurrency and load balance may be improved by assigning rows to tasks in more
    creative ways
    \begin{itemize}
    \item cyclically: assign row $j$ to task $j \mod p$
    \item block cyclically
    \item reflectively
    \end{itemize}
%
  \item overall execution speed depends on the ability to overlap communication with the
    computation of successive steps
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Coarsening by columns}
%
  \begin{itemize}
%
  \item for one dimensional coarsening into $n/p$ rows
    \begin{itemize}
    \item there is no need to broadcast the components of $x$ vertically, but also no
      parallelism in computing the products
    \item horizontal exchanges are still required for the sum reductions that accumulate the
      inner products
    \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
        \dontprintsemicolon
        \nocaptionofalgo
        \setalcaphskip{0ex}
%
        \For{$i=1$ \KwTo $n$}{
          t = 0 \;
          \For{ $j \in mycolumns$, $j < i$}{
            $t = t + L_{ij} x_{j}$
          }
          \If{ $i \in mycolumns$} {
            \KwRecv reduction of $t$ \;
            $x_{i} = (b_{i} -t)/ L_{ii}$
          } \Else {
            \KwSend $t$ for reduction across all tasks
          }
        }
% 
      \end{algorithm}
    \end{minipage}
  \end{center}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% observations on coarsening by columns
\begin{frame}[fragile]
%
  \frametitle{Observations on coarsening by columns}
%
  \begin{itemize}
%
%
  \item load balance:
    \begin{itemize}
    \item tasks are idle until solution component corresponding to their first column is
      computed, and there is progressively less work as column number increases
    \item if a task holds a contiguous block of columns, it may remain idle for most of the
      calculation
    \end{itemize}
%
  \item both concurrency and load balance may be improved by assigning columns to tasks in more
    creative ways
    \begin{itemize}
    \item cyclically: assign column $j$ to task $j \mod p$
    \item block cyclically
    \item reflectively
    \end{itemize}
%
  \item overall execution speed depends on the ability to overlap communication with the
    computation of successive steps
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% wavefront algorithms
\begin{frame}[fragile]
%
  \frametitle{Wavefront algorithms}
%
  \begin{itemize}
%
  \item fan out and fan in algorithm share many characteristics
    \begin{itemize}
    \item parallelism comes from partitioning and distributing the work of the inner loop
    \item while the outer loop is serial
    \item they work on one component of the solution at a time, although one can partially
      pipeline successive steps
    \end{itemize}
%
    \item {\em wavefront} algorithms exploit parallelism in the outer loops by explicitly
      working on multiple components of the solution at the same time
%
    \item consider the one dimensional fan out algorithm
      \begin{itemize}
      \item it appears there is no opportunity for parallelism: after the owner of column $j$
        computes $x_{j}$, the updated components of $b$ cannot shared with other tasks because
        they have no access to column $j$
      \item however, the task that owns column $j$ could finish only a fraction of the work,
        say the first $s$ components, and pass them on to the task that owns column $j+1$,
        before continuing on to the next $s$ components
      \item the task that owns column $j+1$ receives the first $s$ components of $b$, it can
        compute $x_{j+1}$, begin a fraction of the remaining updates and forward its
        contributions to the next task
      \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% wavefront implementation
\begin{frame}[fragile]
%
  \frametitle{Wavefront implementation}
%
  \begin{itemize}
%
  \item we need two new features
    \begin{itemize}
    \item a vector $z$ that accumulates the updates to $b$
    \item the notion of a {\em segment} that contains no more than $s$ consecutive components of $z$
    \end{itemize}
%
  \end{itemize}
%
  \begin{center}
    \begin{minipage}{.85\linewidth}
      \begin{algorithm}[H]
%
          \dontprintsemicolon
          \nocaptionofalgo
          \setalcaphskip{0ex}
%
          \For{$j \in mycolumns$}{
            \For{$k=1$ \KwTo number of segments}{
              \KwRecv $segment$\;
              \If {$k=1$} {
                $x_{j} = (b_{j} - z_{j})/ L_{jj}$ \;
                $segment = segment - \{z_{j}\}$ \;
              }
              \For{$z_{i} \in segment$} {
                $z_{i} = z_{i} + L_{ij}x_{j}$ \;
              }
              \If{$length(segment) > 0$}{
                \KwSend $segment$ \KwTo task owning column $j+1$
              }
            }
          }
%
        \end{algorithm}
      \end{minipage}
    \end{center}
%
\end{frame}



% --------------------------------------
% two dimensional coarsening
\begin{frame}[fragile]
%
  \frametitle{Block coarsening}
%
  \begin{itemize}
%
  \item coalesce $(n/\sqrt{p}) \times (n/\sqrt{p})$ fine grain tasks in to a coarse grain block
%
  \item resulting in an algorithm with features from both column and row coarsening
    \begin{itemize}
    \item both vertical broadcasts and horizontal reductions are required to communicate
      solution components and accumulate inner products
    \end{itemize}
%
  \item the na\"ive implementation assigns contiguous blocks of rows and columns to coarse
    tasks
    \begin{itemize}
    \item poor concurrency and efficiency 
    \item almost half the tasks are idle
    \end{itemize}
%
  \item cyclic assignment, with $L_{ij}$ assigned to task $(i \mod \sqrt{p}, j \mod \sqrt{p})$,
    yields $p$ non-null tasks so the full grid of tasks is active
    \begin{itemize}
    \item again, the obvious implementation where we loop over successive solution components
      to perform horizontal reductions and vertical broadcasts has limited concurrency, since
      computation of each component involves only one task row and one task column
    \item improve by computing solution components in groups of $\sqrt{p}$, which enables all
      tasks to perform the updates concurrently
    \end{itemize}
%
  \end{itemize}
%
\end{frame}



% end of file 
