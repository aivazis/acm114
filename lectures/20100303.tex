% -*- LaTeX -*-
% -*- coding: utf-8 -*-
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
%                             michael a.g. aïvázis
%                      california institute of technology
%                      (c) 1998-2010  all rights reserved
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%

\lecture{Matrices}{20100303}

% --------------------------------------
% dense matrices
\begin{frame}[fragile]
%
  \frametitle{Dense matrix problems}
%
  \begin{itemize}
%
  \item we'll take a look at
    \begin{itemize}
    \item inner and outer products of two vector
    \item matrix-vector and matrix-matrix multiplication
    \item $LU$ factorization and Cholesky decomposition
    \item QR factorization
    \item computing eigenvalues and eigenvectors
    \item fast Fourier transforms
    \end{itemize}
%
  \item when solving a problem of size $n$ on $p$ processors, we will assume
    \begin{itemize}
    \item that $p$, and occasionally $\sqrt{p}$ divides $n$
    \item that $p$ is a perfect square, when forming two-dimensional process grids
    \item matrices are $n\times n$ -- square, not rectangular
    \item we are memory constrained and data replication must be minimized
    \end{itemize}
%
    \item these problems have been studied extensively and form the core of scientific
      computing on parallel machines
      \begin{itemize}
      \item excellent implementations available 
      \item interest has been revived due to the expected disruption by multi-core
        architectures
      \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% vector inner product
\begin{frame}[fragile]
%
  \frametitle{Vector inner product}
%
  \begin{itemize}
%
  \item the inner product of two $n$-vectors $x$, $y$ is given by
    \begin{equation}
    x^{T}y = \sum_{i=1}^{n} x_{i}y_{i}
    \end{equation}
    which requires $n$ multiplications and $n-1$ additions
%
  \item parallelization strategy:
    \begin{itemize}
    \item $n$ fine grain tasks, numbered $i = 1, \ldots, n$, that store $x_{i}$ and $y_{i}$,
      and compute $x_{i} y_{i}$
    \item communication is a sum reduction over $n$ fine grain tasks
    \item coarsening is achieved by coalescing $n/p$ tasks together, assuming that each process
      can accommodate the data storage requirements
    \item and mapping each coarse grain task to a process
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% vector outer product
\begin{frame}[fragile]
%
  \frametitle{Vector outer product}
%
  \begin{itemize}
%
  \item the outer product of two $n$-vectors $x$ and $y$ is the $n \times n$ matrix $A$ given
    by
    \begin{equation}
      A_{ij} = x_{i} y_{j}
    \end{equation}
    which requires $n^{2}$ multiplications
%
  \item parallelization strategies are determined by the storage requirements
    \begin{itemize}
    \item build a two-dimensional grid of $n^{2}$ fine grain tasks numbered $(i,j)$, with $i,j
      = 1, \ldots, n$; each one computes $x_{i}y_{j}$
    \item assuming no data replication is allowed
      \begin{itemize}
      \item let task $(i,1)$ store $x_{i}$ and task $(1,i)$ store $y_{i}$
      \item or, let task $(i,i)$ store both $x_{i}$ and $y_{i}$
      \end{itemize}
    \item either way, the task that owns each element must broadcast it to the other tasks:
      $x_{i}$ along the \th{i} task row, $y_{j}$ along the \th{j} task column
    \item coarsening to $p$ tasks can be accomplished by
      \begin{itemize}
      \item combining $n/p$ rows or columns
      \item forming $(n/\sqrt{p}) \times (n/\sqrt{p})$ grid of fine grain tasks
      \end{itemize}
    \item and each coarse grain task can be assigned to a process
    \end{itemize}
%
  \item either way, na\"ive broadcasting of the components of $x$ and $y$ would require as much
    total memory as replication
    \begin{itemize}
    \item storage can be reduced by circulating portions of $x$ and $y$ through the tasks, with
      each task using the available portion and passing it on
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% matrix vector product
\begin{frame}[fragile]
%
  \frametitle{The product of a matrix with a vector}
%
  \begin{itemize}
%
  \item given an $n \times n$ matrix $A$ and an $n$-vector $x$, the matrix vector product
    yields an $n$-vector $y$ whose components are given by
    \begin{equation}
      y_{i} = \sum_{j=1}^{n} A_{ij} x_{j}
    \end{equation}
    requiring a total of $n^{2}$ multiply-add operations
%
  \item once again, the parallelization strategy is determined by how the data is distributed
    among fine grain tasks
    \begin{itemize}
    \item build a two-dimensional grid of $n^{2}$ fine grain tasks numbered $(i,j)$, with $i,j
      = 1, \ldots, n$; each one computes $A_{ij}x_{j}$
    \item task $(i,j)$ has $A_{i,j}$, but if no data replication is allowed
      \begin{itemize}
      \item let task $(i,1)$ store $x_{i}$ and task $(1,i)$ store $y_{i}$
      \item or, let task $(i,i)$ store both $x_{i}$ and $y_{i}$
      \end{itemize}
    \item the task that owns $x_{j}$ must broadcast it along the \th{j} task row, and $y_{i}$
      is formed by sum reduction along the \th{i} task column
    \item coarsening into $p$ tasks can be accomplished by combining $n/p$ rows/columns, or by
      forming $(n/\sqrt{p}) \times (n/\sqrt{p})$ blocks
    \item and each coarse grain task can be assigned to a process
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening matrix vector product
\begin{frame}[fragile]
%
  \frametitle{Coarsening along rows or columns}
%
  \begin{itemize}
%
  \item for one-dimensional coarsening into $n/p$ task rows
    \begin{itemize}
    \item if $x$ is stored in one task, it must be broadcast to all others
    \item if $x$ is distributed among tasks, with $n/p$ components per task, then multiple
      broadcasts are required
    \item each task computes the inner product of its $n/p$ rows of $A$ with the entire $x$ to
      produce $n/p$ components of $y$
    \end{itemize}
%
  \item for one-dimensional coarsening into $n/p$ task columns
    \begin{itemize}
    \item $n/p$ components of $x$ are distributed among the tasks
    \item each task computes the linear combination of its $n/p$ columns with coefficients
      from its copy of $x$
    \item since the right parts of $x$ are already available, no communication is required
    \item $y$ is generated by a sum reduction across tasks
    \end{itemize}
%
  \item these two are {\em duals} of each other
    \begin{itemize}
    \item row coarsening begins with broadcast, followed by communication-free inner products
    \item column coarsening begins with communication-free linear combinations, follows by a
      reduction
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% 2d coarsening
\begin{frame}[fragile]
%
  \frametitle{Two dimensional coarsening}
%
  \begin{itemize}
%
  \item for two dimensional coarsening, we form $(n/\sqrt{p}) \times (n/\sqrt{p})$ blocks of
    fine grain task
    \begin{itemize}
    \item each one holding a $(n/\sqrt{p}) \times (n/\sqrt{p})$ block of $A$
    \item with components of $x$ distributed either across one task row, or along the diagonal,
      $n/p$ components per task
    \end{itemize}
%
  \item the algorithm combines the features of row/column coarsening
    \begin{itemize}
    \item components of $x$ are broadcast along task columns
    \item each task performs $n^{2}/p$ multiplications locally and sums $n/\sqrt{p}$ sets of
      products
      \item sum reductions along task rows produce the components of $y$ by combining the
        component products
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% matrix-matrix multiply
\begin{frame}[fragile]
%
  \frametitle{Matrix multiplication}
%
  \begin{itemize}
%
  \item the product of two $n \times n$ matrices $A$ and $B$ is an $n \times n$ matrix $C$
    given by
    \begin{equation}
      C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}
    \end{equation}
    where each one of $n^{2}$ entries requires $n$ multiply-adds for a total of $n^{3}$
    operations
%
  \item matrix multiplication can be viewed as
    \begin{itemize}
    \item $n^{2}$ inner products
    \item the sum of $n$ outer products
    \item $n$ matrix vector products
    \end{itemize}
%
  \item each one produces a parallel algorithm for matrix multiplication
%
  \item but we'll explore a direct solution instead
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% matrix-matrix multiply: partitioning and communication
\begin{frame}[fragile]
%
  \frametitle{Partitioning and communication patterns}
%
  \begin{itemize}
%
  \item we build a three dimensional array of $n^{3}$ fine grain tasks
    \begin{itemize}
    \item with $i,j,k = 1,\ldots,n$, let task $(i,j,k)$ be responsible for computing the
      product $A_{ij}B_{jk}$
    \item assuming no data replication, we have to distribute the data for $A$ and $B$ among
      $2n^{2}$ tasks
    \item suppose that task $(i,j,j)$ holds $A_{i,j}$ and task $(i,j,i)$ holds $B_{i,j}$
    \item we will refer to tasks along $i$ and $j$ as task rows and columns
    \item and tasks along $k$ as {\em layers}
    \end{itemize}
%
  \item the communication requirements among tasks are satisfied if we
    \begin{itemize}
    \item broadcast the entries of the \th{k} column of $A$ from task $(i,j,j)$ to each task
      row in the \th{k} layer
    \item broadcast the entries of the \th{k} row of $B$ from task $(i,j,i)$ to each task column of
      the \th{k} layer
    \item form the result $C_{ij}$ by the sum reduction of the values held by all the
      tasks layers $k$
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% coarsening
\begin{frame}[fragile]
%
  \frametitle{Coarsening}
%
  \begin{itemize}
%
  \item there are four natural ways to coarsen our $n \times n \times n$ fine grain tasks into
    $p$ coarse grain tasks
    \begin{itemize}
    \item by task rows: combine the $(n/p) \times n \times n$ tasks along a given task row
    \item by task columns: combine the $n \times (n/p) \times n$ tasks along a given task
      column
    \item partition the layers in a two dimensional grid by combining $(n/\sqrt{p}) \times
      (n/\sqrt{p}) \times n$ fine grain tasks
    \item using three dimensional blocks by combining $(n/\sqrt[3]{p}) \times (n/\sqrt[3]{p})
      \times (n/\sqrt[3]{p})$ tasks
    \end{itemize}
%
    \item the two one dimensional coarsening strategies are similar
      \begin{itemize}
      \item for row coarsening
        \begin{itemize}
        \item each task needs only the part of $A$ it already has, but needs all $B$ entries
        \item so global communication is required to broadcast the $n^{2}/p$ entries of $B$ held
          by each task
        \end{itemize}
      \item conversely, for column coarsening
        \begin{itemize}
        \item each task needs only the parts of $B$ that it already has, but it needs all of $A$
        \item so global communication is required to broadcast the $n^{2}/p$ entries of $A$ held
          by each task
        \end{itemize}
      \item if accumulating $A$ or $B$ on each processor is not feasible, tasks can circulate
        portions of the array in a ring
%
      \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% 2d coarsening
\begin{frame}[fragile]
%
  \frametitle{Coarsening using a two dimensional grid}
%
  \begin{itemize}
%
  \item block matrix multiplication has the same overall form as actual product, with scalar
    operations replaced by the matrix product of blocks!
%
  \item you should verify that
    \begin{equation}
      C_{ij} = \sum_{k=1}^{\sqrt{p}} A_{ik} B_{kj}
    \end{equation}
    for $i,j = 1, \ldots, \sqrt{p}$
%
  \item assume that task $(i,j$) has local access to block $A_{ij}$ and $B_{ij}$ and computes
    block $C_{ij}$ of the result
%
  \item this requires all blocks $A_{ik}$ and $B_{kj}$ for $k=1,\ldots,\sqrt{p}$ to be
    communicated
    \begin{itemize}
    \item first, a global broadcast of $A$ blocks across each task row 
    \item followed by a global broadcast of $B$ blocks across each task column 
    \end{itemize}
%
  \item memory requirements can be addressed by either of the following:
    \begin{itemize}
    \item broadcast blocks of $A$ across rows while circulating blocks of $B$ across columns in
      lock step, so that they arrive at a given task at the same time
    \item circulate blocks of $A$ horizontally and blocks of $B$ vertically, after an initial
      circular shift, so that blocks meet at a given task at the right time
    \end{itemize}
%
  \end{itemize}
%
\end{frame}


% end of file 
