% -*- LaTeX -*-
% -*- coding: utf-8 -*-
%
% michael a.g. aïvázis
% california institute of technology
% (c) 1998-2012 all rights reserved
%

\lecture{More programming with MPI}{20120130}

% --------------------------------------
% messages
\begin{frame}[fragile]
%
  \frametitle{Messages}
%
  \begin{itemize}
%
  \item in general, data exchanges through MPI calls involve
    \begin{itemize}
    \item a communicator
      \begin{itemize}
      \item specifies which processes participate in the exchange
      \item resolves process ranks into processes
      \end{itemize}        
    \item {\em collective} operations involve the entire communicator
    \item {\em point-to-point} operations require the rank of the message source or destination
    \item the details of the message payload
      \begin{itemize}
      \item the address of the source buffer
      \item the data type of the buffer contents
      \item the number of items in the buffer
      \end{itemize}
    \end{itemize}
%
    \item \mpi\ provides some data abstractions to
      \begin{itemize}
      \item hide machine dependencies in the data representations to enhance portability and
        support heterogeneous clusters
      \item support user defined data types
      \item support non-contiguous data layouts
      \end{itemize}        
      
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% global operations
\begin{frame}[fragile]
%
  \frametitle{Collective operations: global reductions}
%
  \begin{itemize}
%
  \item {\em collective} operations involve all processes in a given communicator
%
  \item the \mpi\ version of our global reduction example uses
    \begin{C}
int MPI_Allreduce(
        void* send_buffer, void* recv_buffer,
        int count, MPI_Datatype datatype, MPI_Op operation,
        MPI_Comm communicator
        );
   \end{C}
%
  \item example legal values for \identifier{MPI\_Datatype}
    \begin{itemize}
    \item \cc: \identifier{MPI\_INT}, \identifier{MPI\_LONG}, \identifier{MPI\_DOUBLE} 
    \item \fortran: \identifier{MPI\_INTEGER}, \identifier{MPI\_DOUBLE\_PRECISION},
      \identifier{MPI\_COMPLEX}
    \end{itemize}
%
  \item legal values for \identifier{MPI\_Op}
    \begin{itemize}
    \item \identifier{MPI\_MAX}, \identifier{MPI\_MIN}, \identifier{MPI\_MAXLOC},
      \identifier{MPI\_MINLOC}
    \item \identifier{MPI\_SUM}, \identifier{MPI\_PROD}
    \item \identifier{MPI\_LAND}, \identifier{MPI\_LOR}, \identifier{MPI\_LXOR}
    \item \identifier{MPI\_BAND}, \identifier{MPI\_BOR}, \identifier{MPI\_BXOR}
    \item \identifier{MPI\_REPLACE}
    \end{itemize}
%
  \end{itemize}
%
\end{frame}

% --------------------------------------
% example reduction with mpi
\begin{frame}[fragile]
%
  \frametitle{Example reduction using \mpi}
%
  \label{slide:squares-mpi}
%
  \begin{C}[basicstyle=\tt\bfseries\tiny]
#include <mpi.h>
#include <stdio.h>

int main(int argc, char* argv[]) {
    int status;
    int rank;
    int square, sum;

    /* initialize MPI */
    status = MPI_Init(&argc, &argv);
    if (status != MPI_SUCCESS) {
        printf("error in MPI_Init; aborting...\n");
        return status;
    }

    /* get the process rank */
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    /* form the square */
    square = rank*rank;
    /* each process contributes the square of its rank */
    MPI_Allreduce(&square, &sum, 1, MPI_INT,  MPI_SUM, MPI_COMM_WORLD);
    /* print out the result */
    printf("%03d: sum = %d\n", rank, sum);

    /* shut down MPI */
    MPI_Finalize();

    return 0;
}
  \end{C}
%
\end{frame}

% end of file 
